{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3775e30-c822-4aa0-832c-090ce77ba0ae",
   "metadata": {},
   "source": [
    "# Non-Linear ODE Solvers\n",
    "\n",
    "**Important concepts:**\n",
    "\n",
    "- The Bisection method is robust and slow, while the Newton method is the fastest we can use but requires a good initial guess to converge to the solution.\n",
    "\n",
    "- Backward Euler requires nonlinear solution methods which can be achieved by Newton’s method.\n",
    "\n",
    "- Large time steps can compromise the convergence of Newton’s method.\n",
    "\n",
    "Non-linear differential equations are those in which the unknown function and its derivatives appear in non-linear terms. Alternatively, the relationship between the function, its derivatives, and the independent variables is not linear. Common examples are:\n",
    "\n",
    "- The growth equation is a non-linear first-order ODE:\n",
    "\n",
    "$$\\frac{dy}{dt} = ry\\left(1 - \\frac{y}{K}\\right)$$\n",
    "\n",
    "- The second-order equation for a simple pendulum is also non-linear:\n",
    "\n",
    "$$\\frac{d^2\\theta}{dt^2} + \\frac{g}{L}\\sin(\\theta) = 0$$\n",
    "\n",
    "- The heat-conduction equation:\n",
    "\n",
    "$$\\rho c\\frac{\\partial T}{\\partial t} = \\nabla\\cdot(k\\nabla T) + Q(T)$$\n",
    "\n",
    "- The Navier-Stokes equations:\n",
    "\n",
    "$$\\rho\\left(\\frac{\\partial \\mathbf{u}}{\\partial t} + (\\mathbf{u}\\cdot\\nabla)\\mathbf{u}\\right) = -\\nabla p + \\mu\\nabla^2\\mathbf{u} + \\mathbf{f}$$\n",
    "\n",
    "These equations can be significantly more complex and challenging to solve compared to linear differential equations because of the following reasons:\n",
    "\n",
    "- Non-linearity means that the terms of the unknown function and its derivatives may include products, powers, trigonometric functions, exponential functions, to name a few.\n",
    "\n",
    "- Non-linear differential equations can exhibit a wide range of complex behaviors such as multiple solutions and sensitivity to initial conditions.\n",
    "\n",
    "As a result, closed-form solutions of non-linear differential equations are often unavailable. Therefore, numerical methods like finite difference methods, finite element methods, and numerical integration techniques are frequently used to approximate solutions. Additionally, qualitative analysis, phase portraits, and stability analysis are valuable tools for understanding the behavior of solutions to non-linear differential equations.\n",
    "\n",
    "In summary, non-linear differential equations are essential tools for modeling complex phenomena across various scientific and engineering domains. They challenge researchers and practitioners to develop innovative mathematical and computational techniques for understanding and solving intricate real-world problems.\n",
    "\n",
    "Let’s start with a general equation:\n",
    "\n",
    "$$f(x)=0, \\hspace{15px} x\\in[a,b]$$\n",
    "\n",
    "Solving the above equation is the same as finding a root of the $f(x)$ in the mentioned interval or the location where $f(x)$ crosses the x-axis. We will find this root with two methods.\n",
    "\n",
    "### Supplementary Video\n",
    "\n",
    "Check out this video for an additional explanation of these topics:\n",
    "\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7iRMKc-DPc4?si=JzNFeBfTajjnCYVa\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "</div>\n",
    "\n",
    "## Bisection\n",
    "\n",
    "The Bisection Method, also known as the interval-halving method, binary search method, or dichotomy method (it is known by different names in different fields), is based on Bolzano’s theorem, which states that:\n",
    "\n",
    "‘for any continuous function $f(x)$ in the interval $[a, b]$ such that $f(a)f(b) < 0$, there is $c \\in [a, b]$ for which $f(c) = 0$’.\n",
    "\n",
    "The above sets a strict condition on the use of the bisection method. Once you are certain that $f(a)f(b) < 0$, you can initiate the iterations by finding the value of $f(x)$ at the midpoint of $[a, b]$, let’s call it $x_k$. Now there are 3 possibilities:\n",
    "\n",
    "1. $f(x_k) = 0$, then you have found the root!\n",
    "2. $f(x_k)$ has the same sign as $f(a)$ $(a < b)$. Then replace $a$ with $x_k$ and repeat the mid-point procedure in the interval $[x_k, b]$.\n",
    "3. $f(x_k)$ has the same sign as $f(b)$ where $(a < b)$. Then replace $b$ with $x_k$ and repeat the mid-point procedure in the interval $[a, x_k]$.\n",
    "\n",
    "Keep iterating until you find the root. Here is a graphical representation of the Bisection method. Please note that the function is $F(x)$, and you are trying to find a root in the interval $[a_1, b_1]$.\n",
    "\n",
    "![Bisection Method](assets/bisection_method.png)\n",
    "\n",
    "In Python, you can use code the above with the following guidelines:\n",
    "\n",
    "- Loop over $k$: $x_k = \\frac{a+b}{2}, k \\geq 1$\n",
    "- if $|f(x_k)| < \\epsilon$: break\n",
    "- else if $f(a)f(x_k) > 0$: $a = x_k$\n",
    "- else: $b=x_k$\n",
    "\n",
    "$\\epsilon$ is called the tolerance. One can rarely achieve $0$ in numerical modeling; instead, one sets a tolerance that is close to $0$, depending on the desired accuracy of the solution. In most cases, $\\epsilon = 10^{-6}$ is enough.\n",
    "\n",
    "Despite being straightforward and often reliable, the bisection method has its limitations, starting with what the Bolzano theorem dictates:\n",
    "\n",
    "- Only suitable for finding a single root of a univariate function within a given interval where the function changes sign. The method is not well-suited for multiple roots or roots of multivariate equations.\n",
    "\n",
    "- The method converges linearly, which means that the number of correct digits in the approximation doubles with each iteration. This makes the method steady but slow.\n",
    "\n",
    "- The initial interval must be chosen carefully. A poor choice refers to intervals that have multiple roots or roots, wherein, the method may converge to the wrong root or fail to converge at all, respectively. This also means that the user must manually select the interval.\n",
    "\n",
    "- It only works for real roots in the real Cartesian plane.\n",
    "\n",
    "Now, let’s take a look at another method, which does not suffer from most of the above limitations.\n",
    "\n",
    "## Newton-Raphson Method\n",
    "\n",
    "The Newton-Raphson method is used to find the roots of real-valued functions based on approximating a function by its tangent at a specific point and then iteratively improving that approximation.\n",
    "\n",
    "- Begin with an initial guess $x_0$ that is reasonably close to the actual root of the equation $f(x) = 0$, if known. If unknown, start with a point of your choice.\n",
    "\n",
    "- Compute the tangent line to the function $f(x)$ at the point $(x_0, f(x_0))$, the equation for which is,\n",
    "\n",
    "$$y = f(x_0) + f'(x_0)(x - x_0)$$\n",
    "\n",
    "where $f'(x_0)$ is the derviative of $f(x)$ at $x_0$.\n",
    "\n",
    "- Find the point where the tangent crosses the $x$-axis, or $x_1$, which is an improved approximation of the root (the guess you made):\n",
    "\n",
    "$$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$\n",
    "\n",
    "- Repeat the calculation for the tangent and the location where it crosses the $x$-axis to obtain another approximation of the root. In index notation:\n",
    "\n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n",
    "\n",
    "When do you stop iterating? You simply stop when $|x_{n+1} - x_{n}| < \\epsilon$ (perhaps $10^{-6})$.\n",
    "\n",
    "![Iterating](assets/iterating.png)\n",
    "\n",
    "In the graph above, one begins with the right-most point (shown by the arrowhead) and moves towards the left as dictated by the tangents with the aim of reaching the location where the red line crosses the $x$-axis.\n",
    "\n",
    "**The Newton-Raphson method:**\n",
    "\n",
    "- Converges quickly and can provide highly accurate approximations of roots.\n",
    "\n",
    "- Is applicable to a wide range of functions and can be used for both real and complex roots.\n",
    "\n",
    "- Can be implemented efficiently and is suitable for computer-based calculations.\n",
    "\n",
    "**But the method also has its limitations:**\n",
    "\n",
    "- It may not converge if the initial guess is far from the actual root or if the function has complex behavior near the root.\n",
    "\n",
    "- Convergence to multiple roots or singular points can pose challenges.\n",
    "\n",
    "- It relies on the availability of the derivative of the function, which may not always be readily accessible or computationally efficient to calculate.\n",
    "\n",
    "Notwithstanding, with a proper initial guess and careful consideration of the function's behavior, the Newton-Raphson method is effective at solving non-linear equations through iterative refinement, making it omnipresent in computational engineering.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
